{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > #### **Importante**, acuérdate de cambiar el entorno a uno con GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos el tiempo inicial del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Dataset\n",
    "SOURCE_LANGUAGE = \"en\"\n",
    "TARGET_LANGUAGE = \"es\"\n",
    "SUBSET = False\n",
    "PERCENT_SUBSET = 0.01\n",
    "\n",
    "# Train\n",
    "LR = 1e-5\n",
    "STEPS = 100000\n",
    "EPOCHS = 200\n",
    "END_AT_EPOCHS = True\n",
    "END_AT_STEPS = False\n",
    "GPUS = 1\n",
    "GPU_NUMBER = 0\n",
    "if GPUS > 1:\n",
    "    BS = 1408\n",
    "else:\n",
    "    if SUBSET:\n",
    "        BS = 128\n",
    "    else:\n",
    "        BS = 256 # Con este batch size debería funcionar, pero eres libre de cambiarlo\n",
    "print(f\"BS: {BS}\")\n",
    "LR_SCHEDULER = False\n",
    "SAVE_BEST_MODEL = True\n",
    "STOP_AT_TIME = True\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='nltk')\n",
    "\n",
    "DIM_EMBEDDING = 512\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 6\n",
    "DROPOUT = 0.1\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# Tokenizers\n",
    "UNKNOWN_TOKEN = \"[UNK]\"\n",
    "PADDING_TOKEN = \"[PAD]\"\n",
    "START_OF_SEQUENCE = \"[SOS]\"\n",
    "END_OF_SEQUENCE = \"[EOS]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "MODEL_PATH = #\"/content/drive/MyDrive/path_donde_quieres_guardar_los_resultados\"\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    files = os.listdir(MODEL_PATH)\n",
    "    model_files = [file for file in files if \"transformer\" in file]\n",
    "    if len(model_files) > 0:\n",
    "        name = model_files[0].split(\".\")[0]\n",
    "        STEP0 = int(name.split(\"_\")[-1])\n",
    "        EPOCH0 = int(name.split(\"_\")[-2])\n",
    "    else:\n",
    "        EPOCH0 = 0\n",
    "        STEP0 = 0\n",
    "else:\n",
    "    EPOCH0 = 0\n",
    "    STEP0 = 0\n",
    "print(f\"EPOCH0: {EPOCH0}, STEP0: {STEP0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets tokenizers\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "path_train = #\"/content/drive/MyDrive/path_donde_tienes_opus100_train_croped_3_20\"\n",
    "path_val = #\"/content/drive/MyDrive/path_donde_tienes_opus100_val_croped_3_20\"\n",
    "path_test = #\"/content/drive/MyDrive/path_donde_tienes_opus100_test_croped_3_20\"\n",
    "\n",
    "dataset_raw_train = load_from_disk(path_train)\n",
    "dataset_raw_val = load_from_disk(path_val)\n",
    "dataset_raw_test = load_from_disk(path_test)\n",
    "\n",
    "if SUBSET:\n",
    "    len_dataset_train = len(dataset_raw_train)\n",
    "    len_dataset_val = len(dataset_raw_val)\n",
    "    len_dataset_test = len(dataset_raw_test)\n",
    "    len_subset_train = int(len_dataset_train * PERCENT_SUBSET)\n",
    "    len_subset_val = int(len_dataset_val * PERCENT_SUBSET)\n",
    "    len_subset_test = int(len_dataset_test * PERCENT_SUBSET)\n",
    "    dataset_raw_train = dataset_raw_train.select(range(len_subset_train))\n",
    "    dataset_raw_val = dataset_raw_val.select(range(len_subset_val))\n",
    "    dataset_raw_test = dataset_raw_test.select(range(len_subset_test))\n",
    "\n",
    "len(dataset_raw_train), len(dataset_raw_val), len(dataset_raw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.device_count() > 1 and GPUS > 1:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(f\"cuda:{GPU_NUMBER}\")\n",
    "        print(f\"Using GPU {GPU_NUMBER}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "TOKENIZERS_PATH = #\"/content/drive/MyDrive/path_donde_tienes_los_tokenizers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_source_path = f\"{TOKENIZERS_PATH}/tokenizer_{SOURCE_LANGUAGE}.json\"\n",
    "print(f\"Loading source tokenizer from {tokenizer_source_path}\")\n",
    "tokenizer_source = Tokenizer.from_file(tokenizer_source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_target_path = f\"{TOKENIZERS_PATH}/tokenizer_{TARGET_LANGUAGE}.json\"\n",
    "print(f\"Loading target tokenizer from {tokenizer_target_path}\")\n",
    "tokenizer_target = Tokenizer.from_file(tokenizer_target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longitud máxima de las secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "max_source_sequence_length = 0\n",
    "max_target_sequence_length = 0\n",
    "\n",
    "progress_bar = tqdm.tqdm(range(len(dataset_raw_train)))\n",
    "\n",
    "for i in progress_bar:\n",
    "    source_sequence_length = len(tokenizer_source.encode(dataset_raw_train[i]['translation'][SOURCE_LANGUAGE]).ids)\n",
    "    target_sequence_length = len(tokenizer_target.encode(dataset_raw_train[i]['translation'][TARGET_LANGUAGE]).ids)\n",
    "    if source_sequence_length > max_source_sequence_length:\n",
    "        max_source_sequence_length = source_sequence_length\n",
    "    if target_sequence_length > max_target_sequence_length:\n",
    "        max_target_sequence_length = target_sequence_length\n",
    "\n",
    "max_sequence_len = max(max_source_sequence_length, max_target_sequence_length)\n",
    "max_sequence_len += 2   # Add 2 for the start and end of sequence tokens\n",
    "\n",
    "print(f\"Max source sequence length: {max_source_sequence_length}\")\n",
    "print(f\"Max target sequence length: {max_target_sequence_length}\")\n",
    "print(f\"Max sequence length: {max_sequence_len}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(sequence_len):\n",
    "    if GPUS > 1:\n",
    "        mask = torch.tril(torch.ones((torch.cuda.device_count(), sequence_len, sequence_len)))\n",
    "    else:\n",
    "        mask = torch.tril(torch.ones((1, sequence_len, sequence_len)))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, max_seq_len) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        \n",
    "        # Defining special tokens by using the target language tokenizer\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(START_OF_SEQUENCE)], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(END_OF_SEQUENCE)], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(PADDING_TOKEN)], dtype=torch.int64)\n",
    "        self.unk_token = torch.tensor([tokenizer_tgt.token_to_id(UNKNOWN_TOKEN)], dtype=torch.int64)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Getting the source and target texts from the dataset\n",
    "        src_target_pair = self.dataset[index]['translation']\n",
    "        src_text = src_target_pair[self.src_lang]\n",
    "        tgt_text = src_target_pair[self.tgt_lang]\n",
    "        \n",
    "        # Tokenizing source and target texts \n",
    "        encoder_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        decoder_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "        \n",
    "        # Computing how many padding tokens need to be added to the tokenized texts \n",
    "        encoder_num_padding_tokens = self.max_seq_len - len(encoder_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
    "        decoder_num_padding_tokens = self.max_seq_len - len(decoder_input_tokens) - 1 # Subtracting the '[SOS]' special token\n",
    "        \n",
    "        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n",
    "        # given the current sequence length limit (this will be defined in the config dictionary below)\n",
    "        if encoder_num_padding_tokens < 0 or decoder_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long')\n",
    "         \n",
    "        # Building the encoder input tensor by combining several elements\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token, # inserting the '[SOS]' token\n",
    "                torch.tensor(encoder_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n",
    "                self.eos_token, # Inserting the '[EOS]' token\n",
    "                torch.tensor([self.pad_token] * encoder_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Building the decoder input tensor by combining several elements\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token, # inserting the '[SOS]' token \n",
    "                torch.tensor(decoder_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                torch.tensor([self.pad_token] * decoder_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Creating a label tensor, the expected output for training the model\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(decoder_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                self.eos_token, # Inserting the '[EOS]' token \n",
    "                torch.tensor([self.pad_token] * decoder_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n",
    "        assert encoder_input.size(0) == self.max_seq_len\n",
    "        assert decoder_input.size(0) == self.max_seq_len\n",
    "        assert label.size(0) == self.max_seq_len\n",
    "\n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input, \n",
    "            'label': label,\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BilingualDataset(dataset_raw_train, tokenizer_source, tokenizer_target, SOURCE_LANGUAGE, TARGET_LANGUAGE, max_sequence_len)\n",
    "validation_dataset = BilingualDataset(dataset_raw_val, tokenizer_source, tokenizer_target, SOURCE_LANGUAGE, TARGET_LANGUAGE, max_sequence_len)\n",
    "test_dataset = BilingualDataset(dataset_raw_test, tokenizer_source, tokenizer_target, SOURCE_LANGUAGE, TARGET_LANGUAGE, max_sequence_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BS, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BS, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BS, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases de bajo nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(CustomLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        init.kaiming_uniform_(self.linear.weight, nonlinearity='relu')\n",
    "        if self.linear.bias is not None:\n",
    "            init.zeros_(self.linear.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(CustomEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        init.xavier_uniform_(self.embedding.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embedding = CustomEmbedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_sequence_len, embedding_model_dim):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_model_dim\n",
    "        positional_encoding = torch.zeros(max_sequence_len, self.embedding_dim)\n",
    "        for pos in range(max_sequence_len):\n",
    "            for i in range(0, self.embedding_dim, 2):\n",
    "                positional_encoding[pos, i]     = torch.sin(torch.tensor(pos / (10000 ** ((2 * i) / self.embedding_dim))))\n",
    "                positional_encoding[pos, i + 1] = torch.cos(torch.tensor(pos / (10000 ** ((2 * (i+1)) / self.embedding_dim))))\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        self.register_buffer('positional_encoding', positional_encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * torch.sqrt(torch.tensor(self.embedding_dim))\n",
    "        sequence_len = x.size(1)\n",
    "        x = x + self.positional_encoding[:,:sequence_len]\n",
    "        return x\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        key_trasposed = key.transpose(-1,-2)\n",
    "        product = torch.matmul(query, key_trasposed)\n",
    "        scale = product / torch.sqrt(torch.tensor(self.dim_embedding))\n",
    "        if mask is not None:\n",
    "            scale = scale.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_matrix = torch.softmax(scale, dim=-1)\n",
    "        output = torch.matmul(attention_matrix, value)\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.dim_proyection = dim_embedding // heads\n",
    "        self.heads = heads\n",
    "        self.proyection_Q = CustomLinear(dim_embedding, dim_embedding)\n",
    "        self.proyection_K = CustomLinear(dim_embedding, dim_embedding)\n",
    "        self.proyection_V = CustomLinear(dim_embedding, dim_embedding)\n",
    "        self.scaled_dot_product_attention = ScaledDotProductAttention(self.dim_proyection)\n",
    "        self.attention = CustomLinear(dim_embedding, dim_embedding)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        proyection_Q = self.proyection_Q(Q).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_K = self.proyection_K(K).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_V = self.proyection_V(V).view(batch_size, -1, self.heads, self.dim_proyection)\n",
    "        proyection_Q = proyection_Q.transpose(1,2)\n",
    "        proyection_K = proyection_K.transpose(1,2)\n",
    "        proyection_V = proyection_V.transpose(1,2)\n",
    "        scaled_dot_product_attention = self.scaled_dot_product_attention(proyection_Q, proyection_K, proyection_V, mask=mask)\n",
    "        concat = scaled_dot_product_attention.transpose(1,2).contiguous().view(batch_size, -1, self.dim_embedding)\n",
    "        output = self.attention(concat)\n",
    "        return output\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, dim_embedding):\n",
    "        super().__init__()\n",
    "        self.normalization = nn.LayerNorm(dim_embedding)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return self.normalization(torch.add(x, sublayer))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_embedding, increment=4):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            CustomLinear(dim_embedding, dim_embedding*increment),\n",
    "            nn.ReLU(),\n",
    "            CustomLinear(dim_embedding*increment, dim_embedding)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feed_forward(x)\n",
    "        return x\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, dim_embedding, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = CustomLinear(dim_embedding, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class Softmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class Dropout(torch.nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            return torch.nn.functional.dropout(x, p=self.p)\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clases de medio nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.dropout_1 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_1 = AddAndNorm(dim_embedding)\n",
    "        self.feed_forward = FeedForward(dim_embedding)\n",
    "        self.dropout_2 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_2 = AddAndNorm(dim_embedding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        multi_head_attention = self.multi_head_attention(x, x, x)\n",
    "        dropout1 = self.dropout_1(multi_head_attention)\n",
    "        add_and_norm_1 = self.add_and_norm_1(x, dropout1)\n",
    "        feed_forward = self.feed_forward(add_and_norm_1)\n",
    "        dropout2 = self.dropout_2(feed_forward)\n",
    "        add_and_norm_2 = self.add_and_norm_2(add_and_norm_1, dropout2)\n",
    "        return add_and_norm_2\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, Nx, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(heads, dim_embedding, prob_dropout) for _ in range(Nx)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.dropout_1 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_1 = AddAndNorm(dim_embedding)\n",
    "        self.encoder_decoder_multi_head_attention = MultiHeadAttention(heads, dim_embedding)\n",
    "        self.dropout_2 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_2 = AddAndNorm(dim_embedding)\n",
    "        self.feed_forward = FeedForward(dim_embedding)\n",
    "        self.dropout_3 = Dropout(prob_dropout)\n",
    "        self.add_and_norm_3 = AddAndNorm(dim_embedding)\n",
    "    \n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        Q = x\n",
    "        K = x\n",
    "        V = x\n",
    "        masked_multi_head_attention = self.masked_multi_head_attention(Q, K, V, mask=mask)\n",
    "        dropout1 = self.dropout_1(masked_multi_head_attention)\n",
    "        add_and_norm_1 = self.add_and_norm_1(dropout1, x)\n",
    "\n",
    "        Q = add_and_norm_1\n",
    "        K = encoder_output\n",
    "        V = encoder_output\n",
    "        encoder_decoder_multi_head_attention = self.encoder_decoder_multi_head_attention(Q, K, V)\n",
    "        dropout2 = self.dropout_2(encoder_decoder_multi_head_attention)\n",
    "        add_and_norm_2 = self.add_and_norm_2(dropout2, add_and_norm_1)\n",
    "\n",
    "        feed_forward = self.feed_forward(add_and_norm_2)\n",
    "        dropout3 = self.dropout_3(feed_forward)\n",
    "        add_and_norm_3 = self.add_and_norm_3(dropout3, add_and_norm_2)\n",
    "\n",
    "        return add_and_norm_3\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, heads, dim_embedding, Nx, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(heads, dim_embedding, prob_dropout) for _ in range(Nx)])\n",
    "    \n",
    "    def forward(self, x, encoder_output, mask=None):\n",
    "        for decoder_layer in self.layers:\n",
    "            x = decoder_layer(x, encoder_output, mask)\n",
    "        return x\n",
    "\n",
    "class Linear_and_softmax(nn.Module):\n",
    "    def __init__(self, dim_embedding, vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear = CustomLinear(dim_embedding, vocab_size)\n",
    "        # self.softmax = Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase de alto nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, src_max_seq_len, tgt_max_seq_len, dim_embedding, Nx, heads, prob_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(heads, dim_embedding, Nx, prob_dropout)\n",
    "        self.decoder = Decoder(heads, dim_embedding, Nx, prob_dropout)\n",
    "        self.sourceEmbedding = Embedding(src_vocab_size, dim_embedding)\n",
    "        self.targetEmbedding = Embedding(tgt_vocab_size, dim_embedding)\n",
    "        self.sourcePositional_encoding = PositionalEncoding(src_max_seq_len, dim_embedding)\n",
    "        self.targetPositional_encoding = PositionalEncoding(tgt_max_seq_len, dim_embedding)\n",
    "        self.linear = Linear_and_softmax(dim_embedding, tgt_vocab_size)\n",
    "    \n",
    "    def encode(self, source):\n",
    "        embedding = self.sourceEmbedding(source)\n",
    "        positional_encoding = self.sourcePositional_encoding(embedding)\n",
    "        encoder_output = self.encoder(positional_encoding)\n",
    "        return encoder_output\n",
    "    \n",
    "    def decode(self, encoder_output, target, target_mask):\n",
    "        embedding = self.targetEmbedding(target)\n",
    "        positional_encoding = self.targetPositional_encoding(embedding)\n",
    "        decoder_output = self.decoder(positional_encoding, encoder_output, target_mask)\n",
    "        return decoder_output\n",
    "    \n",
    "    def projection(self, decoder_output):\n",
    "        linear_output = self.linear(decoder_output)\n",
    "        # softmax_output = self.softmax(linear_output)\n",
    "        return linear_output\n",
    "    \n",
    "    def forward(self, source, target, target_mask):\n",
    "        # Encode\n",
    "        embedding_encoder = self.sourceEmbedding(source)\n",
    "        positional_encoding_encoder = self.sourcePositional_encoding(embedding_encoder)\n",
    "        encoder_output = self.encoder(positional_encoding_encoder)\n",
    "\n",
    "        # Decode\n",
    "        embedding_decoder = self.targetEmbedding(target)\n",
    "        positional_encoding_decoder = self.targetPositional_encoding(embedding_decoder)\n",
    "        decoder_output = self.decoder(positional_encoding_decoder, encoder_output, target_mask)\n",
    "\n",
    "        # Projection\n",
    "        proj_output = self.linear(decoder_output)\n",
    "        return proj_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab_size = tokenizer_source.get_vocab_size()\n",
    "target_vocab_size = tokenizer_target.get_vocab_size()\n",
    "src_max_seq_len = max_sequence_len\n",
    "tgt_max_seq_len = max_sequence_len\n",
    "dim_embedding = DIM_EMBEDDING\n",
    "Nx = NUM_LAYERS\n",
    "heads = NUM_HEADS\n",
    "prob_dropout = DROPOUT\n",
    "print(f\"source vocab size: {source_vocab_size}, target vocab size: {target_vocab_size}, source max sequence len: {src_max_seq_len}, target max sequence len: {tgt_max_seq_len}, dim_embedding: {dim_embedding}, heads: {heads}, Nx: {Nx}, prob_dropout: {prob_dropout}\")\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size = source_vocab_size,\n",
    "    tgt_vocab_size = target_vocab_size,\n",
    "    src_max_seq_len = src_max_seq_len,\n",
    "    tgt_max_seq_len = tgt_max_seq_len,\n",
    "    dim_embedding = dim_embedding,\n",
    "    Nx = Nx,\n",
    "    heads = heads,\n",
    "    prob_dropout = prob_dropout,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parallel\n",
    "if torch.cuda.device_count() > 1 and GPUS > 1:\n",
    "    number_gpus = torch.cuda.device_count()\n",
    "    print(f\"Let's use {number_gpus} GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "else:\n",
    "    number_gpus = 1\n",
    "    print(f\"Let's use {number_gpus} GPUs! GPU NUMBER: {GPU_NUMBER}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = create_mask(max_sequence_len)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(\n",
    "    ignore_index = tokenizer_source.token_to_id(PADDING_TOKEN), \n",
    "    label_smoothing = LABEL_SMOOTHING).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step():\n",
    "    def __init__(self):\n",
    "        self.step = 0\n",
    "    \n",
    "    def set_step(self, st):\n",
    "        self.step = st\n",
    "    \n",
    "    def get_step(self):\n",
    "        return int(self.step)\n",
    "\n",
    "class LearningRate():\n",
    "    def __init__(self):\n",
    "        self.lr = 0\n",
    "    \n",
    "    def set_lr(self, l_r_):\n",
    "        self.lr = l_r_\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if LR_SCHEDULER:\n",
    "            return self.lr\n",
    "        else:\n",
    "            return LR\n",
    "\n",
    "actual_step = Step()\n",
    "actual_lr = LearningRate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_lr(step_num, dim_embeding_model=512, warmup_steps=4000):\n",
    "    step_num += 1e-7 # Avoid division by zero\n",
    "    step_num += STEP0\n",
    "    actual_step.set_step(step_num)\n",
    "    step_num_exp = -0.5\n",
    "    warmup_steps_exp = -1.5\n",
    "    dim_embeding_model_exp = -0.93\n",
    "    lr = np.power(dim_embeding_model, dim_embeding_model_exp) * np.minimum(np.power(step_num, step_num_exp), step_num * np.power(warmup_steps, warmup_steps_exp))\n",
    "    actual_lr.set_lr(lr)\n",
    "    return lr\n",
    "\n",
    "lr_lambda = lambda step: calculate_lr(step, dim_embeding_model=dim_embedding, warmup_steps=400)\n",
    "if LR_SCHEDULER:\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, source, tokenizer_tgt, max_len, device, bs):\n",
    "    # Retrieving the indices from the start and end of sequences of the target tokens\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')    # Start of Sentence token index (2)\n",
    "    # eos_idx = tokenizer_tgt.token_to_id('[EOS]')    # End of Sentence token index (3)\n",
    "\n",
    "    # Computing the output of the encoder for the source sequence\n",
    "    encoder_output = model.encode(source)\n",
    "    \n",
    "    # Initializing the decoder input with the Start of Sentence token\n",
    "    decoder_input = torch.empty(bs,1).fill_(sos_idx).type_as(source).to(device)\n",
    "    \n",
    "    # Looping until the 'max_len', maximum length, is reached\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "            \n",
    "        # Building a mask for the decoder input\n",
    "        decoder_mask = create_mask(decoder_input.size(1)).to(device)\n",
    "        \n",
    "        # Calculating the output of the decoder\n",
    "        out = model.decode(encoder_output, decoder_input, decoder_mask)\n",
    "        \n",
    "        # Applying the projection layer to get the probabilities for the next token\n",
    "        prob = model.projection(out[:, -1])\n",
    "\n",
    "        # Selecting token with the highest probability\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        # decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "        decoder_input = torch.cat([decoder_input, next_word.unsqueeze(1)], dim=1)\n",
    "    \n",
    "    if len(decoder_input.shape) == 1:\n",
    "        decoder_input = decoder_input.unsqueeze(0)\n",
    "    elif len(decoder_input.shape) == 3:\n",
    "        decoder_input = decoder_input.squeeze(0)\n",
    "\n",
    "    return decoder_input # Sequence of tokens generated by the decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def validation_loop(model, validation_dataloader, tokenizer_tgt, max_len, device, master_bar):\n",
    "    model.eval() # Setting model to evaluation mode\n",
    "\n",
    "    # Calculating the number of batches in the validation dataset\n",
    "    dataset_size = len(validation_dataloader.dataset)  # Tamaño total del conjunto de datos\n",
    "    batch_size = validation_dataloader.batch_size      # Tamaño del batch\n",
    "    drop_last = validation_dataloader.drop_last        # Configuración de drop_last\n",
    "    num_batches = len(validation_dataloader)           # Número total de batches\n",
    "\n",
    "    # Calculating the total number of samples in the validation dataset\n",
    "    total_samples = batch_size * (num_batches - 1) + min(batch_size, dataset_size % batch_size)\n",
    "\n",
    "    # If drop_last is False and the dataset size is not divisible by the batch size, we need to add one more batch\n",
    "    if drop_last and dataset_size % batch_size != 0:\n",
    "        total_samples -= dataset_size % batch_size\n",
    "\n",
    "    # Initializing progress bar\n",
    "    # progress_bar_ = tqdm(range(total_samples), desc = 'Processing validation examples') # Initializing progress bar\n",
    "    progress_validation_bar = progress_bar(validation_dataloader, parent=master_bar)\n",
    "\n",
    "    # Initializing lists to store scores\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    \n",
    "    # Creating evaluation loop\n",
    "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
    "        for batch in progress_validation_bar:\n",
    "        # for batch in validation_dataloader:\n",
    "            # Loading input data and masks onto the GPU\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            \n",
    "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "            num_samples = len(batch['src_text'])\n",
    "            model_out_bs = greedy_decode(model, encoder_input, tokenizer_tgt, max_len, device, num_samples)\n",
    "\n",
    "            # Get metrics for every example in the batch\n",
    "            for i in range(num_samples):\n",
    "                source_text = batch['src_text'][i]\n",
    "                target_text = batch['tgt_text'][i]\n",
    "                model_out_i = model_out_bs[i]\n",
    "                model_out_text = tokenizer_tgt.decode(model_out_i.detach().cpu().numpy())\n",
    "\n",
    "                # Calculating metrics\n",
    "                references = [target_text.split()]\n",
    "                hypothesis = model_out_text.split()\n",
    "                bleu_score = sentence_bleu(references, hypothesis)\n",
    "                meteor_score_value = meteor_score(references, hypothesis)\n",
    "            \n",
    "                # Appending scores to lists\n",
    "                bleu_scores.append(bleu_score)\n",
    "                meteor_scores.append(meteor_score_value)\n",
    "\n",
    "                # Calculating mean scores            \n",
    "                mean_bleu_score = sum(bleu_scores)/len(bleu_scores) # Calculating mean BLEU score\n",
    "                mean_meteor_score = sum(meteor_scores)/len(meteor_scores) # Calculating mean METEOR score\n",
    "\n",
    "                # Updating progress bar and printing bleu and meteor scores\n",
    "                # progress_bar_.update(1)\n",
    "                # progress_bar_.set_postfix({'BLEU': f'{mean_bleu_score:.9f}', 'METEOR': f'{mean_meteor_score:.9f}'})\n",
    "                master_bar.child.comment = f'BLEU: {mean_bleu_score:.9f}, METEOR: {mean_meteor_score:.9f}'\n",
    "\n",
    "    # Printing results\n",
    "    # console_width = 80 # Fixed witdh for printed messages\n",
    "    # print('-'*console_width)\n",
    "    # print(f'SOURCE: {source_text}')\n",
    "    # print(f'TARGET: {target_text}')\n",
    "    # print(f'PREDICTED: {model_out_text}')\n",
    "    # print('-'*console_width)\n",
    "\n",
    "    return mean_bleu_score, mean_meteor_score, source_text, target_text, model_out_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "def train_loop(model, train_dataloader, device, tokenizer_target, loss_fn, optimizer, master_bar, lr_scheduler=None):\n",
    "    # Initializing an iterator over the training dataloader\n",
    "    # We also use tqdm to display a progress bar\n",
    "    # batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n",
    "    progress_train_bar = progress_bar(train_dataloader, parent=master_bar)\n",
    "    \n",
    "    # For each batch...\n",
    "    # for batch in batch_iterator:\n",
    "    for batch in progress_train_bar:\n",
    "        model.train() # Train the model\n",
    "            \n",
    "        # Loading input data and masks onto the GPU\n",
    "        encoder_input = batch['encoder_input'].to(device)\n",
    "        decoder_input = batch['decoder_input'].to(device)\n",
    "        decoder_mask = create_mask(decoder_input.size(1)).to(device)\n",
    "        \n",
    "        # Running tensors through the Transformer\n",
    "        proj_output = model(encoder_input, decoder_input, decoder_mask)\n",
    "        \n",
    "        # Loading the target labels onto the GPU\n",
    "        label = batch['label'].to(device)\n",
    "        \n",
    "        # Computing loss between model's output and true labels\n",
    "        loss = loss_fn(proj_output.view(-1, tokenizer_target.get_vocab_size()), label.view(-1))\n",
    "        \n",
    "        # Updating progress bar, print loss and lr\n",
    "        # batch_iterator.set_postfix({'loss': f'{loss.item():.6f}', 'lr': f'{actual_lr.get_lr():.9f}'})\n",
    "        if loss.item() < 1e-5:\n",
    "            loss_string = f'{loss.item():e}'\n",
    "        elif loss.item() == 0:\n",
    "            loss_string = '0'\n",
    "        else:\n",
    "            loss_string = f'{loss.item():.6f}'\n",
    "        if actual_lr.get_lr() < 1e-5:\n",
    "            actual_lr_string = f'{actual_lr.get_lr():e}'\n",
    "        elif actual_lr.get_lr() == 0:\n",
    "            actual_lr_string = '0'\n",
    "        else:\n",
    "            actual_lr_string = f'{actual_lr.get_lr():.9f}'\n",
    "        master_bar.child.comment = f'loss: {loss_string}, lr: {actual_lr_string}'\n",
    "        \n",
    "        # Performing backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters based on the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update step and LR\n",
    "        if LR_SCHEDULER:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        # Clearing the gradients to prepare for the next batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    return loss.item(), actual_lr.get_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elapsed_time(t0):\n",
    "    t = time.time() - t0  # tiempo transcurrido en segundos\n",
    "\n",
    "    # Convertir el tiempo a días, horas, minutos y segundos\n",
    "    days, t = divmod(t, 86400)  # 86400 segundos en un día\n",
    "    hours, t = divmod(t, 3600)  # 3600 segundos en una hora\n",
    "    minutes, seconds = divmod(t, 60)  # 60 segundos en un minuto\n",
    "\n",
    "    return int(days), int(hours), int(minutes), int(seconds)\n",
    "\n",
    "def elapsed_seconds(days, hours, minutes, seconds):\n",
    "    return days * 86400 + hours * 3600 + minutes * 60 + seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EPOCH0 != 0 or STEP0 != 0:\n",
    "    weights = f\"{MODEL_PATH}/transformer_{EPOCH0}_{STEP0}.pth\"\n",
    "    print(f\"Loading model from {weights}\")\n",
    "    if GPUS > 1:\n",
    "        model.load_state_dict(torch.load(weights))\n",
    "    else:\n",
    "        model = torch.load(weights, map_location=device)\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            print(\"Transforming model from data parallel to model\")\n",
    "            model = model.module\n",
    "    print(f\"Model loaded from {weights}\")\n",
    "\n",
    "    validation_file_path = f\"{MODEL_PATH}/best_validation_bleu_{EPOCH0}_{STEP0}.npy\"\n",
    "    print(f\"load best validation loss from {validation_file_path}\")\n",
    "    best_bleu = np.load(validation_file_path)\n",
    "    print(f\"best validation meteor: {best_bleu}\")\n",
    "else:\n",
    "    best_bleu = 1e-20\n",
    "\n",
    "print(f\"Starting training from epoch {EPOCH0} and step {STEP0}, best bleu: {best_bleu}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "train_lr_list = []\n",
    "validation_meteor_list = []\n",
    "validation_bleu_list = []\n",
    "\n",
    "train_loss_list = np.array(train_loss_list)\n",
    "train_lr_list = np.array(train_lr_list)\n",
    "validation_meteor_list = np.array(validation_meteor_list)\n",
    "validation_bleu_list = np.array(validation_bleu_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress.fastprogress import master_bar\n",
    "\n",
    "max_seconds = 60*60*11 + 60*30 # 11 horas y 30 minutos\n",
    "\n",
    "num_steps_in_epoch = len(train_dataloader)\n",
    "epochs = STEPS // num_steps_in_epoch\n",
    "if END_AT_STEPS:\n",
    "    if STEPS % num_steps_in_epoch != 0:\n",
    "        epochs += 1\n",
    "else:\n",
    "    epochs = EPOCHS\n",
    "print(f\"Se va a entrenar durante {epochs} epochs\")\n",
    "\n",
    "master_progress_bar = master_bar(range(EPOCH0, epochs))\n",
    "\n",
    "for epoch in master_progress_bar:\n",
    "    days, hours, minutes, seconds = elapsed_time(t0)\n",
    "\n",
    "    # We run the 'train_loop' function at the beginning of each epoch\n",
    "    if LR_SCHEDULER:\n",
    "        train_loss, train_lr = train_loop(model, train_dataloader, device, tokenizer_target, loss_fn, optimizer, master_progress_bar, lr_scheduler)    \n",
    "    else:\n",
    "        train_loss, train_lr = train_loop(model, train_dataloader, device, tokenizer_target, loss_fn, optimizer, master_progress_bar)\n",
    "        \n",
    "    # We run the 'run_validation' function at the end of each epoch to evaluate model performance\n",
    "    validation_meteor, validation_bleu, validation_source_text, validation_target_text, validation_model_out_text = validation_loop(model, validation_dataloader, tokenizer_target, max_sequence_len, device, master_progress_bar)\n",
    "\n",
    "    train_loss_list = np.append(train_loss_list, train_loss)\n",
    "    train_lr_list = np.append(train_lr_list, train_lr)\n",
    "    validation_meteor_list = np.append(validation_meteor_list, validation_meteor)\n",
    "    validation_bleu_list = np.append(validation_bleu_list, validation_bleu)\n",
    "\n",
    "    best_validation_belu_string = None\n",
    "    if (validation_bleu > best_bleu) and SAVE_BEST_MODEL:\n",
    "        best_bleu = validation_bleu\n",
    "        weights = f\"{MODEL_PATH}/transformer_{epoch}_{actual_step.get_step()}.pth\"\n",
    "        if GPUS > 1:\n",
    "            torch.save(model.state_dict(), weights)\n",
    "        else:\n",
    "            torch.save(model, weights)\n",
    "\n",
    "        best_bleu_path = f\"{MODEL_PATH}/best_validation_bleu_{epoch}_{actual_step.get_step()}.npy\"\n",
    "        np.save(best_bleu_path, best_bleu)\n",
    "        best_validation_belu_string = f\"best validation bleu: {best_bleu} at epoch {epoch}\"\n",
    "    \n",
    "    days, hours, minutes, seconds = elapsed_time(t0)\n",
    "    elapsed_time_string = f\"Elapsed time: {days} days, {hours} hours, {minutes} minutes, {seconds} seconds\"\n",
    "    if (elapsed_seconds(days, hours, minutes, seconds) > max_seconds) and STOP_AT_TIME:\n",
    "        print(f\"Elapsed time: {days} days, {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
    "        break\n",
    "\n",
    "    if best_validation_belu_string:\n",
    "        master_progress_bar_string = f\"Epoch {epoch} ----- train loss: {train_loss}, validation meteor: {validation_meteor}, validation bleu: {validation_bleu} ----- {best_validation_belu_string} ----- {elapsed_time_string}\"\n",
    "    else:\n",
    "        master_progress_bar_string = f\"Epoch {epoch} ----- train loss: {train_loss}, validation meteor: {validation_meteor}, validation bleu: {validation_bleu} ----- {elapsed_time_string}\"\n",
    "    master_progress_bar_string += f\" ----- SOURCE: {validation_source_text} ----- TARGET: {validation_target_text} ----- PREDICTED: {validation_model_out_text}\"\n",
    "    master_progress_bar.main_bar.comment = master_progress_bar_string\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(40, 10))\n",
    "axs[0].plot(train_loss_list, 'b*')\n",
    "axs[0].grid()\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[1].plot(validation_meteor_list, 'r*')\n",
    "axs[1].grid()\n",
    "axs[1].set_title(\"Validation meteor\")\n",
    "axs[2].plot(validation_bleu_list, 'g*')\n",
    "axs[2].grid()\n",
    "axs[2].set_title(\"Validation bleu\")\n",
    "axs[3].plot(train_lr_list, 'y*')\n",
    "axs[3].grid()\n",
    "axs[3].set_title(\"Learning rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_list, 'b*')\n",
    "plt.plot(validation_meteor_list*1000, 'r*')\n",
    "plt.plot(validation_bleu_list*100, 'g*')\n",
    "plt.grid()\n",
    "fig = plt.legend([\"Train loss\", \"Val meteor x10^3\", \"Val bleu x10^2\"], loc='upper left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cursopytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
